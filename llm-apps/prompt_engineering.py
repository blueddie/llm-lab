"""
프롬프트 엔지니어링 — 같은 모델, 같은 질문, 다른 프롬프트 → 다른 결과

3가지 기법을 비교:
1. Zero-shot: 그냥 질문만
2. Few-shot: 예시를 보여주고 질문
3. CoT (Chain of Thought): 단계적으로 생각하게 유도

각 기법이 왜 효과적인지, 언제 쓰는지를 실제 결과로 확인
"""

import asyncio
import os

from dotenv import load_dotenv
from openai import AsyncOpenAI

load_dotenv()

client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))


async def call_llm(label: str, messages: list[dict]) -> str:
    """공통 호출 함수"""

    response = await client.chat.completions.create(
        model="gpt-4.1-nano",
        messages=messages,
        max_tokens=500,
    )

    answer = response.choices[0].message.content
    tokens = response.usage.total_tokens
    print(f"=== {label} ({tokens}토큰) ===")
    print(answer)
    print()
    return answer


async def example_system_prompt():
    """
    System Prompt — LLM의 역할과 규칙을 설정

    같은 질문이라도 system prompt에 따라 답변 스타일이 완전히 달라짐
    """

    print("=" * 60)
    print("1. System Prompt — 역할 설정의 힘")
    print("=" * 60 + "\n")

    question = "Python에서 리스트와 튜플의 차이가 뭐야?"

    # system prompt 없이
    await call_llm("system prompt 없음", [
        {"role": "user", "content": question},
    ])

    # 초보자용
    await call_llm("초보자 멘토", [
        {
            "role": "system",
            "content": (
                "너는 프로그래밍을 처음 배우는 사람을 가르치는 멘토야. "
                "전문 용어를 쓰지 말고, 일상 비유로 설명해. "
                "3줄 이내로 답변해."
            ),
        },
        {"role": "user", "content": question},
    ])

    # 시니어 개발자용
    await call_llm("시니어 개발자", [
        {
            "role": "system",
            "content": (
                "너는 10년차 Python 시니어 개발자야. "
                "핵심만 간결하게 답변해. "
                "메모리 구조, 성능 차이 등 내부 동작 관점에서 설명해."
            ),
        },
        {"role": "user", "content": question},
    ])


async def example_few_shot():
    """
    Few-shot — 예시를 보여줘서 출력 형태를 유도

    "이런 식으로 해줘"를 말로 설명하는 것보다
    예시를 보여주는 게 훨씬 정확함
    """

    print("=" * 60)
    print("2. Few-shot — 예시의 힘")
    print("=" * 60 + "\n")

    # Zero-shot: 그냥 질문
    await call_llm("Zero-shot (예시 없음)", [
        {
            "role": "system",
            "content": "기술 용어를 간단한 한 줄 설명으로 바꿔줘.",
        },
        {"role": "user", "content": "Docker"},
    ])

    # Few-shot: 예시를 먼저 보여줌
    await call_llm("Few-shot (예시 2개 제공)", [
        {
            "role": "system",
            "content": "기술 용어를 간단한 한 줄 설명으로 바꿔줘.",
        },
        # 예시 1 — user/assistant 쌍으로 "이렇게 해"를 보여줌
        {"role": "user", "content": "Git"},
        {"role": "assistant", "content": "코드의 타임머신. 언제든 과거로 돌아갈 수 있게 해주는 버전 관리 도구."},
        # 예시 2
        {"role": "user", "content": "API"},
        {"role": "assistant", "content": "프로그램끼리 대화하는 약속된 창구. 메뉴판처럼 '이렇게 요청하면 이걸 줄게'가 정해져 있음."},
        # 실제 질문
        {"role": "user", "content": "Docker"},
    ])


async def example_cot():
    """
    CoT (Chain of Thought) — 단계적 사고 유도

    복잡한 추론이 필요한 질문에서 효과적
    "바로 답하지 말고 단계적으로 생각해" → 정확도가 올라감
    """

    print("=" * 60)
    print("3. CoT — 단계적 사고의 힘")
    print("=" * 60 + "\n")

    question = (
        "서버 3대가 있고, 각 서버는 초당 100개의 요청을 처리할 수 있습니다. "
        "피크 시간에 초당 250개의 요청이 들어오고, "
        "각 서버는 10%의 확률로 다운됩니다. "
        "99% 이상의 요청을 처리하려면 서버가 몇 대 필요한가요?"
    )

    # 그냥 질문
    await call_llm("CoT 없음 (바로 답변)", [
        {"role": "user", "content": question},
    ])

    # CoT: 단계적으로 생각하게 유도
    await call_llm("CoT 적용 (단계적 사고)", [
        {
            "role": "system",
            "content": "문제를 단계별로 나눠서 풀어. 각 단계의 계산 과정을 명확히 보여줘.",
        },
        {"role": "user", "content": question + "\n\n단계별로 생각해서 풀어줘."},
    ])


async def main():
    await example_system_prompt()
    await example_few_shot()
    await example_cot()


if __name__ == "__main__":
    asyncio.run(main())

"""
===========================================================
1. System Prompt — 역할 설정의 힘
============================================================

=== system prompt 없음 (520토큰) ===
Python에서 리스트(List)와 튜플(Tuple)은 둘 다 여러 개의 값을 저장할 수 있는 데이터 타입이지만, 몇 가지 중요한 차이가 있습니다.

1. 변경 가능 여부 (Mutability)
   - 리스트(List): 변경 가능 (mutable) — 생성한 후 값 추가, 수정, 삭제가 가능합니다.
   - 튜플(Tuple): 변경 불가 (immutable) — 한 번 생성된 후 변경이 불가능합니다.
2. 사용 목적
   - 리스트는 데이터를 가변적으로 다루거나, 값을 자주 바꿔야 할 때 사용합니다.
   - 튜플은 데이터의 불변성을 보장하거나, 변경이 필요 없는 고정된 데이터를 저장할 때 사용합니다.
3. 문법
   - 리스트: 대괄호 `[ ]` 사용
     ```python
     my_list = [1, 2, 3]
     ```
   - 튜플: 소괄호 `( )` 또는 구문 특성상 괄호 없이 값들을 나열
     ```python
     my_tuple = (1, 2, 3)
     # 또는
     my_tuple = 1, 2, 3
     ```
4. 성능
   - 튜플이 리스트보다 약간 더 빠르고, 메모리 사용량이 적습니다.
5. 메서드 지원
   - 리스트는 `append()`, `remove()`, `sort()`, `reverse()` 등 다양한 변경 관련 메서드를 지원합니다.
   - 튜플은 `count()`, `index()` 등 읽기 전용 메서드만 지원하며, 값 수정 메서드는 없습니다.

요약하면:
| 특징             | 리스트 (List)                         | 튜플 (Tuple)                          |
|------------------|-------------------------------------|------------------------------------|
| 변경 가능 여부   | 변경 가능 (mutable)                | 변경 불가 (immutable)             |
| 문법             | `[ ... ]`                         | `( ... )` 또는 값 나열             |
| 용도             | 가변 데이터, 자주 변경 필요 시    | 불변 데이터, 수정 방지 필요 시    |
| 성능             | 느림 (리스트 > 튜플)               | 빠름, 적은 메모리 사용            |
| 지원 메서드      | 많음 (`append()`, `remove()`, 등) | 적음 (`

=== 초보자 멘토 (134토큰) ===
리스트는 마치 노트에 적어두는 계획처럼 바꿀 수 있어. 반면에 튜플은 수정할 수 없는 다이어리처럼 한 번 적은 것을 바꿀 수 없어. 그러니, 변경이 필요하면 리스트, 아니면 튜플을
써!

=== 시니어 개발자 (177토큰) ===
리스트는 가변(mutable)이며, 내부에서 크기와 내용 변경 가능. 튜플은 불변(immutable)로, 한 번 생성 후 내용 변경 불가. 메모리 면에서는 튜플이 더 작고, 해시 가능(딕셔너리 키 가
능). 성능 면에서는 읽기/생성은 유사하지만, 튜플이 더 빠름. 내부적으로 리스트는 가변 배열, 튜플은 고정 크기 연속 메모리 구조 사용.

============================================================
2. Few-shot — 예시의 힘
============================================================

=== Zero-shot (예시 없음) (54토큰) ===
Docker: 소프트웨어를 쉽게 배포하고 실행할 수 있게 도와주는 컨테이너 기술입니다.

=== Few-shot (예시 2개 제공) (136토큰) ===
가상화 기술로 애플리케이션과 그 환경을 하나로 묶어 어디서든 쉽게 실행할 수 있게 하는 도구.

============================================================
3. CoT — 단계적 사고의 힘
============================================================

=== CoT 없음 (바로 답변) (575토큰) ===
먼저, 문제의 조건을 정리해보겠습니다:

- 각 서버는 초당 100개의 요청을 처리할 수 있다.
- 들어오는 요청이 피크 시간에 초당 250개.
- 각 서버는 10%의 확률로 다운된다.
- 요청 처리율을 99% 이상으로 보장하려면 어떤 서버 개수가 적합한지 계산해야 함.

---

### 1. 기본 서버 수가 요청량을 커버하는지 확인

요청량: 250개/초
한 서버 처리량: 100개/초
---

### 2. 서버 개수로 요청량 커버하기

**서버 개수 \( N \)개일 때**:

- 요청 처리 가능량 = \( N \times 100 \)개/초

이 요청량이 적어도 250개 이상이어야 하므로:

\[
N \times 100 \geq 250 \Rightarrow N \geq 2.5
\]

즉, 최소 3대는 필요하나, 이때는 **서버 가용률**을 고려해야 합니다.

---

### 3. 서버 다운 확률 및 가용률 고려

- 각 서버는 10% 확률로 다운됩니다.
- 서버가 정상인 경우의 확률: 90% = 0.9

서버가 \( N \)대고, 이 중 몇 대가 정상일지는 **이항 분포**로 모델링할 수 있습니다.

- 서버가 정상인 서버 수: \( K \sim \text{Binomial}(N, 0.9) \)

- 요청 처리 가능량: \( K \times 100 \)

필요 조건: 처리 요청이 250 이상이기 위해, 정상 서버 수가 적어도:

\[
K \times 100 \geq 250 \Rightarrow K \geq 2.5
\]

즉, 정상 서버 개수 \( K \geq 3 \).

---

### 4. 정상 서버 개수에 대한 확률 계산

요구하는 것은, **서버 3대, 4대, ... 등에서 최소 99% 이상 확률로** 요청을 처리할 수 있는 경우입니다.

즉,

\[
P(K \geq 3) \geq 0.99
\]

으로 만족하는

=== CoT 적용 (단계적 사고) (613토큰) ===
이 문제를 단계별로 해결하기 위해 다음과 같이 나누어 생각하겠습니다:

1. 요청 처리량 계산
2. 서버의 가용성 (다운 확률 포함)
3. 필요한 서버 대수 계산
---

### 1. 요청 처리량 계산

- 각 서버는 초당 100개의 요청을 처리할 수 있음.
- 피크 시간에 초당 250개의 요청이 들어옴.

즉, 현재 서버 가용성으로 처리할 수 있는 요청량은:

\[
\text{처리 가능한 요청} = \text{서버 대수} \times 100
\]

### 2. 서버의 가용성 고려

- 각 서버는 10%의 확률로 다운됨.
- 따라서, 각 서버가 정상적으로 동작할 확률은:

\[
P(\text{서버 정상}) = 1 - 0.1 = 0.9
\]

- 서버 여러 대가 동시에 작동하는 상태에서, **신뢰성을 높이기 위해서** 일반적으로 "가용 서버"는 "작동하는 서버의 개수"를 고려해서 계산.

즉, 각각의 서버가 정상일 확률이 0.9이므로, **실제로 동작하는 서버의 기대값**은:

\[
E(\text{가용 서버 수}) = N \times 0.9
\]
(여기서 \(N\)은 서버 대수)

- 그러나, 서비스 신뢰성을 99% 이상 확보하려면, "모든 서버가 정상"일 확률이 99% 이상이어야 하는 것이 아니라, **요청을 처리하는 서버의 가용성**을 확보하는 게 목적인 것 같습니
다.

그래서, **요청처리 성공 확률을 계산하기 위해**:

- 서버가 **적어도** \(k\)대 정상 작동할 확률이 99% 이상이어야 함.

---

### 3. 요구하는 신뢰도와 서버 수 계산

현재 요구 조건은:

> **전체 요청의 99% 이상을 처리해야 함**

즉, 서버가 다운되어도 처리 실패가 1% 이하여야 함.

#### 문제의 핵심 가정:

- **요청 처리 실패**는 서버가 다운되어 요청을 못 처리하는 경우임.
- 요청 처리 성공률을 확보

"""